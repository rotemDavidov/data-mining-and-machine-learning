# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sRooDtTsmh6UHupRpbC6b7OjsmnOUr3w

2.	Open and read 3 the files 'Eliot.txt','Tolkien.txt', and HP_Small.txt 
3.	Concatenate the text files into a single string.
4.	Do the text preprocessing without changing the text size (do not remove extra blanks).
5.	Calculate the reduced word frequency matrix.
"""

# import regular expressins packge
# import numbers package
import re
import numpy as np
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt 
from sklearn.metrics import silhouette_samples, silhouette_score

def readFile(fileName):
    file = open(fileName,'r',encoding="cp437")
    fileStr = ""
    for line in file:
        fileStr += line
    return fileStr
        
# Remove extra spaces
# Remove non-letter chars    
# Change to lower 
def preProcess(fileStr):
    fileStr = re.sub("[^a-zA-Z ]"," ", fileStr)
    #fileStr = re.sub(" +"," ", fileStr)
    fileStr = fileStr.lower()
    return fileStr

#Divide the file in chuncks of the same size wind
def partitionStr(fileStr, windSize):
    numParts = len(fileStr)//windSize#// is getting an integer
    chunks=[]
    for i in range(0, numParts):
        windLoc = i*windSize
        chunks += [fileStr[windLoc:windLoc+windSize]]#important to put dpuble [[]]
    return chunks;

# Count the number of dictionary words in files - Frequency Matrix
def getWordFrequency(chunks,dictionary,rows):
    wordFreq = np.empty((rows,len(dictionary)),dtype=np.int64)
    for i in range(rows):
        print(i)
        for j,word in enumerate(dictionary):        
            wordFreq[i,j] = len(re.findall(word,chunks[i]))
    return wordFreq 
       
numFiles = 3
fileContent = [""]*numFiles

#read  and preprocess files 
fileContent[0] = preProcess(readFile('Eliot.txt'))
fileContent[1] = preProcess(readFile('Tolkien.txt'))
fileContent[2] = preProcess(readFile('HP_Small.txt'))



allFilesStr = ""
for i in range(numFiles):
    allFilesStr += fileContent[i]

#wind - chunks size 
wind = 5000

chunks = partitionStr(allFilesStr , wind)    

rows = len(chunks)
# Construct dictionary lines 54 - 65 


# Generate a set of all words in files 
wordsSet =  set(allFilesStr.split())

# Read stop words file - words that can be removed
stopWordsSet = set(readFile('stopwords_en.txt').split())
# Remove the stop words from the word list
dictionary = wordsSet.difference(stopWordsSet)

wordFrequency = getWordFrequency(chunks,dictionary,rows)

# find the sum of the frequency colomns and select colomns having sum > 100
minSum = 100
sumArray =  wordFrequency.sum(axis=0)
indexArray = np.where(sumArray > minSum)

indexArraySize = len(indexArray[0])
wordFrequency1 = np.empty((rows,indexArraySize),dtype=np.int64)

# generate a frequencey file with the selected coloumns 
for j in range(indexArraySize):
    wordFrequency1[:,j] = wordFrequency[:,indexArray[0][j]]
#load file dist1.npy(from lab3_ex011.py)


#dist=np.load('dist.npy')
num_clusters = 3
#cluster the data into k clusters, specify the k  
kmeans = KMeans(n_clusters = num_clusters)
kmeans.fit(wordFrequency1)
labels = kmeans.labels_ + 1
#show the clustering results  

plt.bar(range(len(labels)),labels) 
plt.title("The partitions cluster labels")
plt.xlabel("The number of partition")
plt.ylabel("The cluster label")
plt.show()

# calculate the silhouette values  
silhouette_avg = silhouette_score(wordFrequency, labels)
sample_silhouette_values = silhouette_samples(wordFrequency, labels)
# show the silhouette values 
plt.plot(sample_silhouette_values) 
plt.title("The silhouette plot")
plt.xlabel("The number of partition")
plt.ylabel("The silhouette coefficient values")

xmin=0
xmax=len(labels)
# The vertical line for average silhouette score of all the values
plt.hlines(silhouette_avg, xmin, xmax, colors='red', linestyles="--") 
plt.show()

print("The number of clusters =", num_clusters,
  "The average silhouette score is:", silhouette_avg)

"""6.	Use the k-means algorithm to find the books boundaries. Show the clustering results in subplots.
7.	Partition the text back into its original 3 different strings. The partition point might be in a small interval that is not greater than 5000 characters..
9.	Write a function that will automatically find the boundaries between the books in the text files. 
10.	Using the found boundaries, save each book from the original concatenated text (before text filtering) in different files, using the write() function. 
11.	Describe your function and the results. 



"""

# import regular expressins packge
# import numbers package
import re
import numpy as np
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt 
from sklearn.metrics import silhouette_samples, silhouette_score

def readFile(fileName):
    file = open(fileName,'r',encoding="cp437")
    fileStr = ""
    for line in file:
        fileStr += line
    return fileStr
        
# Remove extra spaces
# Remove non-letter chars    
# Change to lower 
def preProcess(fileStr):
    fileStr = re.sub("[^a-zA-Z ]"," ", fileStr)
    #fileStr = re.sub(" +"," ", fileStr)
    fileStr = fileStr.lower()
    return fileStr

#Divide the file in chuncks of the same size wind
def partitionStr(fileStr, windSize):
    numParts = len(fileStr)//windSize#// is getting an integer
    chunks=[]
    for i in range(0, numParts):
        windLoc = i*windSize
        chunks += [fileStr[windLoc:windLoc+windSize]]#important to put dpuble [[]]
    return chunks;

# Count the number of dictionary words in files - Frequency Matrix
def getWordFrequency(chunks,dictionary,rows):
    wordFreq = np.empty((rows,len(dictionary)),dtype=np.int64)
    for i in range(rows):
        print(i)
        for j,word in enumerate(dictionary):        
            wordFreq[i,j] = len(re.findall(word,chunks[i]))
    return wordFreq 
       
numFiles = 3
fileContent = [""]*numFiles

#read  and preprocess files 
fileContent[0] = preProcess(readFile('Eliot.txt'))
fileContent[1] = preProcess(readFile('Tolkien.txt'))
fileContent[2] = preProcess(readFile('HP_Small.txt'))

#wind - chunks size 
wind = 5000
#Divide the each file into chunks of the size wind 
chunks = []
for i in range(numFiles):
    chunks+= partitionStr(fileContent[i] , wind)    

rows = len(chunks)
# Construct dictionary lines 54 - 65 

allFilesStr = ""
for i in range(numFiles):
    allFilesStr += fileContent[i]

# Generate a set of all words in files 
wordsSet =  set(allFilesStr.split())

# Read stop words file - words that can be removed
stopWordsSet = set(readFile('stopwords_en.txt').split())
# Remove the stop words from the word list
dictionary = wordsSet.difference(stopWordsSet)

wordFrequency = getWordFrequency(chunks,dictionary,rows)

# find the sum of the frequency colomns and select colomns having sum > 100
minSum = 100
sumArray =  wordFrequency.sum(axis=0)
indexArray = np.where(sumArray > minSum)

indexArraySize = len(indexArray[0])
wordFrequency1 = np.empty((rows,indexArraySize),dtype=np.int64)

# generate a frequencey file with the selected coloumns 
for j in range(indexArraySize):
    wordFrequency1[:,j] = wordFrequency[:,indexArray[0][j]]
#load file dist1.npy(from lab3_ex011.py)

#dist=np.load('dist.npy')
num_clusters = 3
#cluster the data into k clusters, specify the k  
kmeans = KMeans(n_clusters = num_clusters)
kmeans.fit(wordFrequency1)
labels = kmeans.labels_ + 1
#show the clustering results  

plt.bar(range(len(labels)),labels) 
plt.title("The partitions cluster labels")
plt.xlabel("The number of partition")
plt.ylabel("The cluster label")
plt.show()

# calculate the silhouette values  
silhouette_avg = silhouette_score(wordFrequency, labels)
sample_silhouette_values = silhouette_samples(wordFrequency, labels)
# show the silhouette values 
plt.plot(sample_silhouette_values) 
plt.title("The silhouette plot")
plt.xlabel("The number of partition")
plt.ylabel("The silhouette coefficient values")

xmin=0
xmax=len(labels)
# The vertical line for average silhouette score of all the values
plt.hlines(silhouette_avg, xmin, xmax, colors='red', linestyles="--") 
plt.show()

print("The number of clusters =", num_clusters,
  "The average silhouette score is:", silhouette_avg)




#################################################### MY CODE ##########################################################


i=0
clean_list = list(dict.fromkeys(labels))
def change_val(i,labels): 
  cnt=0
  j=i+1
  while j<len(labels)-2 and labels[j]==labels[j+1]: #search for at least 4 value to consider starting,ending,continue point
      cnt+=1
      j+=1
  if cnt<4:
    j=i+1
    cnt=cnt+j
    while j<=cnt and j<len(labels):
      labels[j]=-1
      j+=1
  return labels,j

while i<len(labels)-1:
  if labels[i]!=labels[i+1]:
    labels,i = change_val(i,labels)
  else:
    i+=1

############################# create the string of the text without the filter ################################################

numFiles = 3
fileContent = [""]*numFiles

#read  and preprocess files 
fileContent[0] = readFile('Eliot.txt')
fileContent[1] = readFile('Tolkien.txt')
fileContent[2] = readFile('HP_Small.txt')


allFilesStr = ""
for i in range(numFiles):
    allFilesStr += fileContent[i]
################################################################################################################################


for i in clean_list:
    index_mat = numpy.where(labels==i)[0] #return tuple so ive done [0]
    begin = index_mat[0]*5000 #the first elemnt is the first time we saw the index
    end = index_mat[len(index_mat)-1]*5000 #the last time we saw the index
    if not begin:
      print("Book " +str(i)+" begins in character 0")
    else:
      print( "Book " + str(i) + " start in the interval between "+ str(begin) + " and " + str(begin+5000) + " characters ")
    print( "Book " + str(i) + " ends in the interval between "+ str(end) + " and " + str(end+5000) + " characters ")
    with open("file "+str(i), 'w') as f:
      f.write( allFilesStr[begin:end+5000])